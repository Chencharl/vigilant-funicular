1. Change history 1
accuracy: 0.9448684210526316
Change:
    Add:
    lr_scheduler_type="cosine",  
    warmup_steps=500

    #r=8, 
    r=16,
    #lora_alpha=16, 
    lora_alpha=32,
    #target_modules=["query", "value"], 
    target_modules=["value"], 
    lora_dropout=0.1, 
    bias="none", 
    task_type=TaskType.SEQ_CLS ) 

2. Change history 2  
accuracy: TBD (training in progress)  
Change:  
    Add:  
    Per-layer LoRA injection strategy with dual-stage configuration  
        - encoder.layer.0~5:  
            r=8  
            lora_alpha=16  
            target_modules=["value"]  
        - encoder.layer.6~11:  
            r=16  
            lora_alpha=32  
            target_modules=["query", "value"]  
    lora_dropout=0.1  
    bias="none"  
    task_type=TaskType.SEQ_CLS  
    Resulting trainable parameters: 962,308
