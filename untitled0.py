# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Tr4e0xr-3x3k5uaKlo7BcmF625YiXhR
"""

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
from datasets import load_dataset, Dataset
from sklearn.metrics import accuracy_score
import numpy as np
from peft import get_peft_model, LoraConfig, TaskType
import pandas as pd
import pickle

# Test GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Load the AG News dataset
dataset = load_dataset("ag_news")

# Tokenizer（Use RoBERTa）
tokenizer = AutoTokenizer.from_pretrained("roberta-base")
def tokenize_function(example):
  return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)

tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset = tokenized_dataset.rename_column("label", "labels")
tokenized_dataset.set_format("torch", columns=["input_ids", "attention_mask", "labels"])

# Load the RoBERTa model and add the LoRA adapter
model = AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=4)
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["query", "value"],
    lora_dropout=0.1,
    bias="none",
    task_type=TaskType.SEQ_CLS )

model = get_peft_model(model, lora_config)
model.to(device)
model.print_trainable_parameters()

from peft import get_peft_model_state_dict

lora_params = sum(p.numel() for p in get_peft_model_state_dict(model).values())
print(f"✅ LoRA adapter trainable parameters: {lora_params:,}")

# ✅ 第 6 步：训练参数设置
training_args = TrainingArguments(
    output_dir="./results",
    eval_steps=500,
    save_strategy="no",
    learning_rate=2e-4,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    report_to="none" )

def compute_metrics(eval_pred):
  logits, labels = eval_pred
  preds = np.argmax(logits, axis=1)
  return {"accuracy": accuracy_score(labels, preds)}

# ✅ 第 7 步：训练模型
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    compute_metrics=compute_metrics, )

trainer.train()

# ✅ 第 8 步：评估模型
eval_results = trainer.evaluate()
print("Final Evaluation Accuracy:", eval_results["eval_accuracy"])

# ✅ 加载测试集
with open("/content/test_unlabelled.pkl", "rb") as f: test_dataset = pickle.load(f)
# ✅ 转为 HuggingFace Dataset 格式
test_dataset = Dataset.from_dict({"text": test_dataset["text"]})
# ✅ Tokenize 测试集
def preprocess_function(examples):
  return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)

tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)
tokenized_test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask"])

# ✅ 批量预测
from torch.utils.data import DataLoader
test_dataloader = DataLoader(tokenized_test_dataset, batch_size=64)
model.eval()
all_predictions = []

with torch.no_grad():
  for batch in test_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    outputs = model(**batch)
    preds = torch.argmax(outputs.logits, dim=-1)
    all_predictions.extend(preds.cpu().numpy())

# ✅ 保存预测结果为 submission.csv
df = pd.DataFrame({
  "ID": list(range(len(all_predictions))),
  "label": all_predictions })
df.to_csv("submission.csv", index=False)
print("Save Successfully：submission.csv")